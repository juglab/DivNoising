{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DivNoising - Training\n",
    "This notebook contains an example on how to train a DivNoising VAE.  This requires having a noise model (model of the imaging noise) which can be either measured from calibration data or estimated from raw noisy images themselves. If you haven't done so, please first run either ```0a-CreateNoiseModel.ipynb``` (if you have calibration data to create a noise model from) or the ntebook ```0b-CreateNoiseModel.ipynb``` if you do not have calibration data and you wish to use noisy images themselves to bootstrap a noise model. These notebooks will download the data and create a noise model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We import all our dependencies.\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from tifffile import imread\n",
    "from divnoising import utils, training\n",
    "from divnoising.gaussianMixtureNoiseModel import GaussianMixtureNoiseModel\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "if not torch.cuda.is_available():\n",
    "    raise ValueError(\"GPU not found, code will run on CPU and can be extremely slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify ```path``` to load data\n",
    "Your data should be stored in the directory indicated by ```path```. This notebook expects 2D datasets in ```.tif``` format. If your data is a stack of 2D images, you can load it as shown in the next cell. If you dataset has multiple individual 2D tif files, comment out the second line in the cell below and uncomment the third line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./data/Convallaria_diaphragm/\"\n",
    "observation= imread(path+'20190520_tl_25um_50msec_05pc_488_130EM_Conv.tif')\n",
    "# observation= imread(path+'*.tif') # To load multiple individual 2D tif images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we need to follow some preprocessing steps first which will prepare the data for training purposes. <br>\n",
    "We first divide the data randomly into training and validation sets with 85% images allocated to training set  and rest to validation set. <br>\n",
    "Then we augment the training data 8-fold by 90 degree rotations and flips, set `augment` flag to `False` to disable data augmentation. <br>\n",
    "After that patches of size `patch_size` are extracted from training and validation images. <br>\n",
    "Finally, we compute the mean and standard deviation of our combined train and validation sets and do some additional preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches, val_patches = utils.get_trainval_patches(observation,augment=True,patch_size=128)\n",
    "x_train_tensor, x_val_tensor, data_mean, data_std = utils.preprocess(train_patches, val_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure DivNoising model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify some parameters of our DivNoising network needed for training. The default parameters work well for most datasets.<br> \n",
    "\n",
    "$(i)$ The parameter <code>n_depth</code> specifies the depth of the network. <br> \n",
    "$(ii)$ <code>batch_size</code> specifies the batch size used for training. <br>\n",
    "$(iii)$ <code>max_epochs</code> specifies the maximum number of training epochs. In practice, the training may termionate earlier if the validation loss does not improve for $30$ epochs. This is called [early stopping](https://keras.io/api/callbacks/early_stopping/). Currently, we have set ```max_epochs``` heuristically to compute it depending on the number of training patches such that 22 million steps will be taken in total with the entire data seen in each epoch. <br>\n",
    "$(iv)$<code>model_name</code> specifies the name of the model with which the weights will be saved for prediction later. <br>\n",
    "$(v)$<code>basedir</code> is the directory where the model and some logs will be saved during training. If this directory does not exist already, it will be created automatically. <br>\n",
    "$(vi)$ Set <code>real_noise</code> to ```True``` if your dataset is intrinsically noisy. If your dataset is however corrupted synthetically by Gaussian noise, set this parameter to ```False```. If you choose ```real_noise=True```, then you will need to specify a noise model which you should have obtained by running either the notebook ```0a-CreateNoiseModel.ipynb``` or ```0b-CreateNoiseModel.ipynb```. If you have not yet generated the noise model for this dataset yet run that notebook first. \n",
    "If you choose ```real_noise=False```, then you should specify the standard deviation of the Gaussian noise used for synethtically corrupting the data. <br>\n",
    "$(vii)$<code>noise_model</code> is the noise model you want to use. For real microscopy datasets which are intrinsically noisy, first run the notebook  ```0-CreateNoiseModel.ipynb```, if you have not yet generated the noise model for this dataset yet. <br>\n",
    "$(viii)$<code>gaussian_noise_std</code> is only applicable if the dataset was synthetically corrupted with Gaussian noise. In this case, this parameter should be set to the standard deviation of the Gaussian noise used for synthetically corrupting the data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_depth=2\n",
    "batch_size=32\n",
    "max_epochs=int(22000000/(x_train_tensor.shape[0]))\n",
    "model_name = 'divnoising_convallaria_demo' # a name used to identify the model\n",
    "basedir = 'models' # the base directory in which our model will live\n",
    "real_noise=True\n",
    "\n",
    "if real_noise:\n",
    "    noise_model_params= np.load(\"./data/Convallaria_diaphragm/GMMNoiseModel_convallaria_3_2_calibration.npz\")\n",
    "    noise_model = GaussianMixtureNoiseModel(params = noise_model_params, device = device)\n",
    "    gaussian_noise_std = None\n",
    "else:\n",
    "    gaussian_noise_std = 25 # set it to the correct value if using data synthetically corrupted by Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Tensorboard\n",
    "Running the cell below will start a tensorboard instance within this notebook where we can monitor training progress after it starts. If you additionally wish to see tensorboard in a separate browser tab, run the cell first and go to the link http://localhost:6006/  with your browser. The advantage of viewing Tensorboard in a separate tab is that more screen space is allocated to Tensorboard, hence leading to a nicer visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network\n",
    "Running the cell below trains the network and saves the model weights into the directory specified by ```basedir``` earlier. The best and the last models are saved along with tensorboard logs.\n",
    "\n",
    "The number of network parameters and GPU/CPU usage information can be printed by setting the ```log_info``` parameter to ```True```. By default, it is turned off.\n",
    "\n",
    "__Note:__ We observed that for certain datasets, it may happen that the training is getting aborted multiple times with the message `posterior collapse: aborting`. \n",
    "This happens when the KL loss goes towards 0. This phenomenon is called ```posterior collapse``` and is undesirable.\n",
    "We try to prevent it by automatically checking the KL loss and aborting the training and restarting another training run once the KL drops below a threshold (```1e-7```). <br>\n",
    "\n",
    "But if this happens for $20$ times, we start a procedure called [kl annealing](https://arxiv.org/abs/1511.06349) which is a way to avoid ```posterior collapse```. In this case, we increase the weight on KL divergence loss term from 0 to 1 gradually in a number of epochs (the default number of epochs for kl annealing is set to $10$ epochs). With kl annealing, we attempt the training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training.train_network(x_train_tensor, x_val_tensor, batch_size, data_mean, data_std, \n",
    "                       gaussian_noise_std, noise_model, n_depth=n_depth, max_epochs=max_epochs, \n",
    "                       model_name=model_name, basedir=basedir, log_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lightning",
   "language": "python",
   "name": "lightning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
